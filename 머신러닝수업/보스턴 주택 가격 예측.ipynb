{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b46bd34",
   "metadata": {},
   "source": [
    "### 1. 목표\n",
    "- 보스턴에 있는 주택들의 데이터를 바탕으로 집 가격을 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787a2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d8097",
   "metadata": {},
   "source": [
    "### 데이터수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d31fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings #데이터불러올때 warning 안보이게\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911a6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston() #딕셔너리형태로 구성되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b76d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69288cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys() #학습데이터, 정답(결과), 컬럼명, 설명, 파일네임, 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a8e00c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28982986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351cc44",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb3153",
   "metadata": {},
   "source": [
    "### 4. 탐색적 데이터분석(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e3601f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(boston['data'], columns = boston['feature_names']) #학습데이터 데이터프레임 형태로\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240eabc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price\n",
       "0     24.0\n",
       "1     21.6\n",
       "2     34.7\n",
       "3     33.4\n",
       "4     36.2\n",
       "..     ...\n",
       "501   22.4\n",
       "502   20.6\n",
       "503   23.9\n",
       "504   22.0\n",
       "505   11.9\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(boston['target'],columns=['price']) #target이 정답값(주택가격)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b44f8bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f751cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   506 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba307887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29c59864",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0) #데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c56b6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n",
      "(102, 13)\n",
      "(102, 1)\n"
     ]
    }
   ],
   "source": [
    "# x,y 학습데이터의 갯수가 같은지 확인\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df0a16",
   "metadata": {},
   "source": [
    "### 5. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae434c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error #평균제곱오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebc4f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression() #모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "046060b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train, y_train) #모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "123f36d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19443447e-01,  4.47799511e-02,  5.48526168e-03,\n",
       "         2.34080361e+00, -1.61236043e+01,  3.70870901e+00,\n",
       "        -3.12108178e-03, -1.38639737e+00,  2.44178327e-01,\n",
       "        -1.09896366e-02, -1.04592119e+00,  8.11010693e-03,\n",
       "        -4.92792725e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델의 가중치 확인(coef_) - 각 컬럼별로 가중치\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "504580ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.09169493])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#편향값 확인\n",
    "lr.intercept_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757b0b4",
   "metadata": {},
   "source": [
    "### 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2116803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lr.predict(x_train) #train데이터로 예측\n",
    "test = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8b68cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = mean_squared_error(train, y_train) #예측값이랑 정답값 넣고 MSE 확인\n",
    "test_score = mean_squared_error(test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4699460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.326470203585725\n",
      "33.44897999767653\n"
     ]
    }
   ],
   "source": [
    "#mse\n",
    "print(train_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9bcaeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 4.396188144698282\n",
      "rmse: 5.783509315085135\n"
     ]
    }
   ],
   "source": [
    "#rmse\n",
    "print('rmse:', train_score**0.5)\n",
    "print('rmse:', test_score**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8f0826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.387321170386853\n"
     ]
    }
   ],
   "source": [
    "#둘의 오차\n",
    "print(train_score**0.5 -test_score**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e66e86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7730135569264234"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#r^2 비율확인\n",
    "lr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fe7a880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892223849182507"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576aa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800be226",
   "metadata": {},
   "source": [
    "### 선형회귀 모델 특성확장\n",
    "- 선형회귀는 특성이 적으면 다른 알고리즘에 비해 성능이 낮게 나오기 때문에 특성확장을 사용해서 모델의 복잡도를 증가\n",
    "- 각 컬럼들의 데이터를 곱해서 새로운 특성으로 확장을 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02b067dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_train = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "850cbd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "220     17.4  391.70   9.71  \n",
       "71      19.2  376.94   9.88  \n",
       "240     16.6  391.25  11.38  \n",
       "6       15.2  395.60  12.43  \n",
       "417     20.2  127.36  26.64  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97db82e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\1057071582.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n"
     ]
    }
   ],
   "source": [
    "# extended_x_train의 각 컬럼들을 서로 한번씩 곱해서 새로운 컬럼 추가\n",
    "for col1 in x_train.columns : #13번 반복(컬럼 13개)\n",
    "    for col2 in x_train.columns : #13번 반복 더 해야 169개 + 13개\n",
    "        extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3203a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     ...  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  LSTATxDIS  LSTATxRAD  \\\n",
       "220  ...        9.71    4.92297   67.49421    859.335  27.787107      77.68   \n",
       "71   ...        0.00    4.08044   58.89468    172.900  52.238524      39.52   \n",
       "240  ...        0.00    4.87064   78.48786    617.934  72.104818      68.28   \n",
       "6    ...        0.00    6.51332   74.72916    827.838  69.117015      62.15   \n",
       "417  ...        0.00   18.08856  141.29856   2373.624  43.889400     639.36   \n",
       "\n",
       "     LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220    2980.97        168.954  3803.4070      94.2841  \n",
       "71     3013.40        189.696  3724.1672      97.6144  \n",
       "240    3414.00        188.908  4452.4250     129.5044  \n",
       "6      3865.73        188.936  4917.3080     154.5049  \n",
       "417   17742.24        538.128  3392.8704     709.6896  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fef2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) #모든컬럼 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbca514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIMxCRIM</th>\n",
       "      <th>CRIMxZN</th>\n",
       "      <th>CRIMxINDUS</th>\n",
       "      <th>CRIMxCHAS</th>\n",
       "      <th>CRIMxNOX</th>\n",
       "      <th>CRIMxRM</th>\n",
       "      <th>CRIMxAGE</th>\n",
       "      <th>CRIMxDIS</th>\n",
       "      <th>CRIMxRAD</th>\n",
       "      <th>CRIMxTAX</th>\n",
       "      <th>CRIMxPTRATIO</th>\n",
       "      <th>CRIMxB</th>\n",
       "      <th>CRIMxLSTAT</th>\n",
       "      <th>ZNxCRIM</th>\n",
       "      <th>ZNxZN</th>\n",
       "      <th>ZNxINDUS</th>\n",
       "      <th>ZNxCHAS</th>\n",
       "      <th>ZNxNOX</th>\n",
       "      <th>ZNxRM</th>\n",
       "      <th>ZNxAGE</th>\n",
       "      <th>ZNxDIS</th>\n",
       "      <th>ZNxRAD</th>\n",
       "      <th>ZNxTAX</th>\n",
       "      <th>ZNxPTRATIO</th>\n",
       "      <th>ZNxB</th>\n",
       "      <th>ZNxLSTAT</th>\n",
       "      <th>INDUSxCRIM</th>\n",
       "      <th>INDUSxZN</th>\n",
       "      <th>INDUSxINDUS</th>\n",
       "      <th>INDUSxCHAS</th>\n",
       "      <th>INDUSxNOX</th>\n",
       "      <th>INDUSxRM</th>\n",
       "      <th>INDUSxAGE</th>\n",
       "      <th>INDUSxDIS</th>\n",
       "      <th>INDUSxRAD</th>\n",
       "      <th>INDUSxTAX</th>\n",
       "      <th>INDUSxPTRATIO</th>\n",
       "      <th>INDUSxB</th>\n",
       "      <th>INDUSxLSTAT</th>\n",
       "      <th>CHASxCRIM</th>\n",
       "      <th>CHASxZN</th>\n",
       "      <th>CHASxINDUS</th>\n",
       "      <th>CHASxCHAS</th>\n",
       "      <th>CHASxNOX</th>\n",
       "      <th>CHASxRM</th>\n",
       "      <th>CHASxAGE</th>\n",
       "      <th>CHASxDIS</th>\n",
       "      <th>CHASxRAD</th>\n",
       "      <th>CHASxTAX</th>\n",
       "      <th>CHASxPTRATIO</th>\n",
       "      <th>CHASxB</th>\n",
       "      <th>CHASxLSTAT</th>\n",
       "      <th>NOXxCRIM</th>\n",
       "      <th>NOXxZN</th>\n",
       "      <th>NOXxINDUS</th>\n",
       "      <th>NOXxCHAS</th>\n",
       "      <th>NOXxNOX</th>\n",
       "      <th>NOXxRM</th>\n",
       "      <th>NOXxAGE</th>\n",
       "      <th>NOXxDIS</th>\n",
       "      <th>NOXxRAD</th>\n",
       "      <th>NOXxTAX</th>\n",
       "      <th>NOXxPTRATIO</th>\n",
       "      <th>NOXxB</th>\n",
       "      <th>NOXxLSTAT</th>\n",
       "      <th>RMxCRIM</th>\n",
       "      <th>RMxZN</th>\n",
       "      <th>RMxINDUS</th>\n",
       "      <th>RMxCHAS</th>\n",
       "      <th>RMxNOX</th>\n",
       "      <th>RMxRM</th>\n",
       "      <th>RMxAGE</th>\n",
       "      <th>RMxDIS</th>\n",
       "      <th>RMxRAD</th>\n",
       "      <th>RMxTAX</th>\n",
       "      <th>RMxPTRATIO</th>\n",
       "      <th>RMxB</th>\n",
       "      <th>RMxLSTAT</th>\n",
       "      <th>AGExCRIM</th>\n",
       "      <th>AGExZN</th>\n",
       "      <th>AGExINDUS</th>\n",
       "      <th>AGExCHAS</th>\n",
       "      <th>AGExNOX</th>\n",
       "      <th>AGExRM</th>\n",
       "      <th>AGExAGE</th>\n",
       "      <th>AGExDIS</th>\n",
       "      <th>AGExRAD</th>\n",
       "      <th>AGExTAX</th>\n",
       "      <th>AGExPTRATIO</th>\n",
       "      <th>AGExB</th>\n",
       "      <th>AGExLSTAT</th>\n",
       "      <th>DISxCRIM</th>\n",
       "      <th>DISxZN</th>\n",
       "      <th>DISxINDUS</th>\n",
       "      <th>DISxCHAS</th>\n",
       "      <th>DISxNOX</th>\n",
       "      <th>DISxRM</th>\n",
       "      <th>DISxAGE</th>\n",
       "      <th>DISxDIS</th>\n",
       "      <th>DISxRAD</th>\n",
       "      <th>DISxTAX</th>\n",
       "      <th>DISxPTRATIO</th>\n",
       "      <th>DISxB</th>\n",
       "      <th>DISxLSTAT</th>\n",
       "      <th>RADxCRIM</th>\n",
       "      <th>RADxZN</th>\n",
       "      <th>RADxINDUS</th>\n",
       "      <th>RADxCHAS</th>\n",
       "      <th>RADxNOX</th>\n",
       "      <th>RADxRM</th>\n",
       "      <th>RADxAGE</th>\n",
       "      <th>RADxDIS</th>\n",
       "      <th>RADxRAD</th>\n",
       "      <th>RADxTAX</th>\n",
       "      <th>RADxPTRATIO</th>\n",
       "      <th>RADxB</th>\n",
       "      <th>RADxLSTAT</th>\n",
       "      <th>TAXxCRIM</th>\n",
       "      <th>TAXxZN</th>\n",
       "      <th>TAXxINDUS</th>\n",
       "      <th>TAXxCHAS</th>\n",
       "      <th>TAXxNOX</th>\n",
       "      <th>TAXxRM</th>\n",
       "      <th>TAXxAGE</th>\n",
       "      <th>TAXxDIS</th>\n",
       "      <th>TAXxRAD</th>\n",
       "      <th>TAXxTAX</th>\n",
       "      <th>TAXxPTRATIO</th>\n",
       "      <th>TAXxB</th>\n",
       "      <th>TAXxLSTAT</th>\n",
       "      <th>PTRATIOxCRIM</th>\n",
       "      <th>PTRATIOxZN</th>\n",
       "      <th>PTRATIOxINDUS</th>\n",
       "      <th>PTRATIOxCHAS</th>\n",
       "      <th>PTRATIOxNOX</th>\n",
       "      <th>PTRATIOxRM</th>\n",
       "      <th>PTRATIOxAGE</th>\n",
       "      <th>PTRATIOxDIS</th>\n",
       "      <th>PTRATIOxRAD</th>\n",
       "      <th>PTRATIOxTAX</th>\n",
       "      <th>PTRATIOxPTRATIO</th>\n",
       "      <th>PTRATIOxB</th>\n",
       "      <th>PTRATIOxLSTAT</th>\n",
       "      <th>BxCRIM</th>\n",
       "      <th>BxZN</th>\n",
       "      <th>BxINDUS</th>\n",
       "      <th>BxCHAS</th>\n",
       "      <th>BxNOX</th>\n",
       "      <th>BxRM</th>\n",
       "      <th>BxAGE</th>\n",
       "      <th>BxDIS</th>\n",
       "      <th>BxRAD</th>\n",
       "      <th>BxTAX</th>\n",
       "      <th>BxPTRATIO</th>\n",
       "      <th>BxB</th>\n",
       "      <th>BxLSTAT</th>\n",
       "      <th>LSTATxCRIM</th>\n",
       "      <th>LSTATxZN</th>\n",
       "      <th>LSTATxINDUS</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.128228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>38.4400</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>548.700</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>49.60</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>107.880</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.7</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.257049</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>4.056</td>\n",
       "      <td>155.649</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>6.951</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>48.316401</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>55.608</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>548.700</td>\n",
       "      <td>88.5</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>7832.25</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>708.0</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>859.335</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>8.189327</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.056</td>\n",
       "      <td>55.608</td>\n",
       "      <td>708.0</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>139.2</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>77.68</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>307.0</td>\n",
       "      <td>155.649</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.880</td>\n",
       "      <td>17.4</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>139.2</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>302.76</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>168.954</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>391.7</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>153428.8900</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>116.8561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>189.175</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>43.24</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>207.552</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170569</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>1.652</td>\n",
       "      <td>125.965</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>35.533521</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>23.844</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>306.25</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>336.00</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>172.900</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>27.955541</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652</td>\n",
       "      <td>23.844</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>39.52</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.965</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>93025.0</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>336.00</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>76.8</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>368.64</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>189.696</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>142083.7636</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>900.00</td>\n",
       "      <td>147.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.84</td>\n",
       "      <td>206.91</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>341.400</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>147.900</td>\n",
       "      <td>24.3049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>267.699</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>81.838</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>12.84</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.183184</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>2.568</td>\n",
       "      <td>128.400</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>206.91</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>47.568609</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>41.382</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>267.699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>2948.49</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>325.8</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>901.38</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>617.934</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>40.146163</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>180.0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.568</td>\n",
       "      <td>41.382</td>\n",
       "      <td>325.8</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>99.6</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>68.28</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.400</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>498.0</td>\n",
       "      <td>81.838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>901.38</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>99.6</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>275.56</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>188.908</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>153076.5625</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>341.400</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>156.25</td>\n",
       "      <td>98.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>75.15</td>\n",
       "      <td>832.5</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>155.375</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>98.375</td>\n",
       "      <td>61.9369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>524.142</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>39.35</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>119.624</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>6.55</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274576</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>2.620</td>\n",
       "      <td>162.964</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>75.15</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>36.144144</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>30.060</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>832.5</td>\n",
       "      <td>524.142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>4435.56</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>333.0</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>827.838</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>30.919160</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>62.5</td>\n",
       "      <td>39.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620</td>\n",
       "      <td>30.060</td>\n",
       "      <td>333.0</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>62.15</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.964</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>96721.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>190.0</td>\n",
       "      <td>119.624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>231.04</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>188.936</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>156499.3600</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>155.375</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "      <td>672.914728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.6100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>434.40</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>365.620</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461041</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>16.296</td>\n",
       "      <td>452.214</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>28.132416</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>127.296</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>7938.81</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2.714256</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.296</td>\n",
       "      <td>127.296</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>576.0</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>484.8</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.214</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>443556.0</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>484.8</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>408.04</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>16220.5696</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT   CRIMxCRIM   CRIMxZN  CRIMxINDUS  CRIMxCHAS  \\\n",
       "220     17.4  391.70   9.71    0.128228  0.000000    2.220158    0.35809   \n",
       "71      19.2  376.94   9.88    0.025205  0.000000    1.716196    0.00000   \n",
       "240     16.6  391.25  11.38    0.012835  3.398700    0.558520    0.00000   \n",
       "6       15.2  395.60  12.43    0.007795  1.103625    0.694842    0.00000   \n",
       "417     20.2  127.36  26.64  672.914728  0.000000  469.524860    0.00000   \n",
       "\n",
       "      CRIMxNOX     CRIMxRM     CRIMxAGE   CRIMxDIS   CRIMxRAD     CRIMxTAX  \\\n",
       "220   0.181552    2.489084    31.690965   1.024746    2.86472    109.93363   \n",
       "71    0.065568    0.946368     2.778300   0.839412    0.63504     48.42180   \n",
       "240   0.048488    0.781361     6.151647   0.717817    0.67974     33.98700   \n",
       "6     0.046264    0.530799     5.880114   0.490937    0.44145     27.45819   \n",
       "417  17.613667  137.588942  2311.307460  42.737139  622.57440  17276.43960   \n",
       "\n",
       "     CRIMxPTRATIO       CRIMxB  CRIMxLSTAT   ZNxCRIM   ZNxZN  ZNxINDUS  \\\n",
       "220      6.230766   140.263853    3.477054  0.000000    0.00     0.000   \n",
       "71       3.048192    59.842994    1.568549  0.000000    0.00     0.000   \n",
       "240      1.880614    44.324713    1.289240  3.398700  900.00   147.900   \n",
       "6        1.342008    34.927524    1.097445  1.103625  156.25    98.375   \n",
       "417    524.000120  3303.794816  691.057584  0.000000    0.00     0.000   \n",
       "\n",
       "     ZNxCHAS  ZNxNOX   ZNxRM  ZNxAGE     ZNxDIS  ZNxRAD  ZNxTAX  ZNxPTRATIO  \\\n",
       "220      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "71       0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "240      0.0   12.84  206.91  1629.0  190.08300   180.0  9000.0       498.0   \n",
       "6        0.0    6.55   75.15   832.5   69.50625    62.5  3887.5       190.0   \n",
       "417      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "\n",
       "        ZNxB  ZNxLSTAT  INDUSxCRIM  INDUSxZN  INDUSxINDUS  INDUSxCHAS  \\\n",
       "220      0.0     0.000    2.220158     0.000      38.4400         6.2   \n",
       "71       0.0     0.000    1.716196     0.000     116.8561         0.0   \n",
       "240  11737.5   341.400    0.558520   147.900      24.3049         0.0   \n",
       "6     4945.0   155.375    0.694842    98.375      61.9369         0.0   \n",
       "417      0.0     0.000  469.524860     0.000     327.6100         0.0   \n",
       "\n",
       "     INDUSxNOX  INDUSxRM  INDUSxAGE  INDUSxDIS  INDUSxRAD  INDUSxTAX  \\\n",
       "220    3.14340  43.09620    548.700  17.742540      49.60    1903.40   \n",
       "71     4.46453  64.43841    189.175  57.155713      43.24    3297.05   \n",
       "240    2.11004  34.00221    267.699  31.236973      29.58    1479.00   \n",
       "6      4.12388  47.31444    524.142  43.761135      39.35    2447.57   \n",
       "417   12.28990  96.00240   1612.710  29.819750     434.40   12054.60   \n",
       "\n",
       "     INDUSxPTRATIO    INDUSxB  INDUSxLSTAT  CHASxCRIM  CHASxZN  CHASxINDUS  \\\n",
       "220        107.880  2428.5400      60.2020    0.35809      0.0         6.2   \n",
       "71         207.552  4074.7214     106.8028    0.00000      0.0         0.0   \n",
       "240         81.838  1928.8625      56.1034    0.00000      0.0         0.0   \n",
       "6          119.624  3113.3720      97.8241    0.00000      0.0         0.0   \n",
       "417        365.620  2305.2160     482.1840    0.00000      0.0         0.0   \n",
       "\n",
       "     CHASxCHAS  CHASxNOX  CHASxRM  CHASxAGE  CHASxDIS  CHASxRAD  CHASxTAX  \\\n",
       "220        1.0     0.507    6.951      88.5    2.8617       8.0     307.0   \n",
       "71         0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "240        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "6          0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "417        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "\n",
       "     CHASxPTRATIO  CHASxB  CHASxLSTAT   NOXxCRIM  NOXxZN  NOXxINDUS  NOXxCHAS  \\\n",
       "220          17.4   391.7        9.71   0.181552    0.00    3.14340     0.507   \n",
       "71            0.0     0.0        0.00   0.065568    0.00    4.46453     0.000   \n",
       "240           0.0     0.0        0.00   0.048488   12.84    2.11004     0.000   \n",
       "6             0.0     0.0        0.00   0.046264    6.55    4.12388     0.000   \n",
       "417           0.0     0.0        0.00  17.613667    0.00   12.28990     0.000   \n",
       "\n",
       "      NOXxNOX    NOXxRM  NOXxAGE   NOXxDIS  NOXxRAD  NOXxTAX  NOXxPTRATIO  \\\n",
       "220  0.257049  3.524157  44.8695  1.450882    4.056  155.649       8.8218   \n",
       "71   0.170569  2.461893   7.2275  2.183655    1.652  125.965       7.9296   \n",
       "240  0.183184  2.951916  23.2404  2.711851    2.568  128.400       7.1048   \n",
       "6    0.274576  3.150288  34.8984  2.913702    2.620  162.964       7.9648   \n",
       "417  0.461041  3.601416  60.4989  1.118653   16.296  452.214      13.7158   \n",
       "\n",
       "         NOXxB  NOXxLSTAT     RMxCRIM   RMxZN  RMxINDUS  RMxCHAS    RMxNOX  \\\n",
       "220  198.59190    4.92297    2.489084    0.00  43.09620    6.951  3.524157   \n",
       "71   155.67622    4.08044    0.946368    0.00  64.43841    0.000  2.461893   \n",
       "240  167.45500    4.87064    0.781361  206.91  34.00221    0.000  2.951916   \n",
       "6    207.29440    6.51332    0.530799   75.15  47.31444    0.000  3.150288   \n",
       "417   86.47744   18.08856  137.588942    0.00  96.00240    0.000  3.601416   \n",
       "\n",
       "         RMxRM    RMxAGE     RMxDIS   RMxRAD    RMxTAX  RMxPTRATIO  \\\n",
       "220  48.316401  615.1635  19.891677   55.608  2133.957    120.9474   \n",
       "71   35.533521  104.3175  31.517595   23.844  1818.105    114.4512   \n",
       "240  47.568609  374.5071  43.700082   41.382  2069.100    114.4902   \n",
       "6    36.144144  400.3992  33.429726   30.060  1869.732     91.3824   \n",
       "417  28.132416  472.5864   8.738340  127.296  3532.464    107.1408   \n",
       "\n",
       "           RMxB   RMxLSTAT     AGExCRIM  AGExZN  AGExINDUS  AGExCHAS  AGExNOX  \\\n",
       "220  2722.70670   67.49421    31.690965     0.0    548.700      88.5  44.8695   \n",
       "71   2246.93934   58.89468     2.778300     0.0    189.175       0.0   7.2275   \n",
       "240  2698.45125   78.48786     6.151647  1629.0    267.699       0.0  23.2404   \n",
       "6    2378.34720   74.72916     5.880114   832.5    524.142       0.0  34.8984   \n",
       "417   675.51744  141.29856  2311.307460     0.0   1612.710       0.0  60.4989   \n",
       "\n",
       "       AGExRM  AGExAGE    AGExDIS  AGExRAD  AGExTAX  AGExPTRATIO      AGExB  \\\n",
       "220  615.1635  7832.25  253.26045    708.0  27169.5      1539.90  34665.450   \n",
       "71   104.3175   306.25   92.52775     70.0   5337.5       336.00   6596.450   \n",
       "240  374.5071  2948.49  344.05023    325.8  16290.0       901.38  21244.875   \n",
       "6    400.3992  4435.56  370.32930    333.0  20712.6      1012.32  26346.960   \n",
       "417  472.5864  7938.81  146.79225   2138.4  59340.6      1799.82  11347.776   \n",
       "\n",
       "     AGExLSTAT   DISxCRIM     DISxZN  DISxINDUS  DISxCHAS   DISxNOX  \\\n",
       "220    859.335   1.024746    0.00000  17.742540    2.8617  1.450882   \n",
       "71     172.900   0.839412    0.00000  57.155713    0.0000  2.183655   \n",
       "240    617.934   0.717817  190.08300  31.236973    0.0000  2.711851   \n",
       "6      827.838   0.490937   69.50625  43.761135    0.0000  2.913702   \n",
       "417   2373.624  42.737139    0.00000  29.819750    0.0000  1.118653   \n",
       "\n",
       "        DISxRM    DISxAGE    DISxDIS  DISxRAD    DISxTAX  DISxPTRATIO  \\\n",
       "220  19.891677  253.26045   8.189327  22.8936   878.5419     49.79358   \n",
       "71   31.517595   92.52775  27.955541  21.1492  1612.6265    101.51616   \n",
       "240  43.700082  344.05023  40.146163  38.0166  1900.8300    105.17926   \n",
       "6    33.429726  370.32930  30.919160  27.8025  1729.3155     84.51960   \n",
       "417   8.738340  146.79225   2.714256  39.5400  1097.2350     33.27950   \n",
       "\n",
       "           DISxB  DISxLSTAT   RADxCRIM  RADxZN  RADxINDUS  RADxCHAS  RADxNOX  \\\n",
       "220  1120.927890  27.787107    2.86472     0.0      49.60       8.0    4.056   \n",
       "71   1992.994862  52.238524    0.63504     0.0      43.24       0.0    1.652   \n",
       "240  2478.999125  72.104818    0.67974   180.0      29.58       0.0    2.568   \n",
       "6    2199.733800  69.117015    0.44145    62.5      39.35       0.0    2.620   \n",
       "417   209.825600  43.889400  622.57440     0.0     434.40       0.0   16.296   \n",
       "\n",
       "      RADxRM  RADxAGE  RADxDIS  RADxRAD  RADxTAX  RADxPTRATIO    RADxB  \\\n",
       "220   55.608    708.0  22.8936     64.0   2456.0        139.2  3133.60   \n",
       "71    23.844     70.0  21.1492     16.0   1220.0         76.8  1507.76   \n",
       "240   41.382    325.8  38.0166     36.0   1800.0         99.6  2347.50   \n",
       "6     30.060    333.0  27.8025     25.0   1555.0         76.0  1978.00   \n",
       "417  127.296   2138.4  39.5400    576.0  15984.0        484.8  3056.64   \n",
       "\n",
       "     RADxLSTAT     TAXxCRIM  TAXxZN  TAXxINDUS  TAXxCHAS  TAXxNOX    TAXxRM  \\\n",
       "220      77.68    109.93363     0.0    1903.40     307.0  155.649  2133.957   \n",
       "71       39.52     48.42180     0.0    3297.05       0.0  125.965  1818.105   \n",
       "240      68.28     33.98700  9000.0    1479.00       0.0  128.400  2069.100   \n",
       "6        62.15     27.45819  3887.5    2447.57       0.0  162.964  1869.732   \n",
       "417     639.36  17276.43960     0.0   12054.60       0.0  452.214  3532.464   \n",
       "\n",
       "     TAXxAGE    TAXxDIS  TAXxRAD   TAXxTAX  TAXxPTRATIO      TAXxB  TAXxLSTAT  \\\n",
       "220  27169.5   878.5419   2456.0   94249.0       5341.8  120251.90    2980.97   \n",
       "71    5337.5  1612.6265   1220.0   93025.0       5856.0  114966.70    3013.40   \n",
       "240  16290.0  1900.8300   1800.0   90000.0       4980.0  117375.00    3414.00   \n",
       "6    20712.6  1729.3155   1555.0   96721.0       4727.2  123031.60    3865.73   \n",
       "417  59340.6  1097.2350  15984.0  443556.0      13453.2   84821.76   17742.24   \n",
       "\n",
       "     PTRATIOxCRIM  PTRATIOxZN  PTRATIOxINDUS  PTRATIOxCHAS  PTRATIOxNOX  \\\n",
       "220      6.230766         0.0        107.880          17.4       8.8218   \n",
       "71       3.048192         0.0        207.552           0.0       7.9296   \n",
       "240      1.880614       498.0         81.838           0.0       7.1048   \n",
       "6        1.342008       190.0        119.624           0.0       7.9648   \n",
       "417    524.000120         0.0        365.620           0.0      13.7158   \n",
       "\n",
       "     PTRATIOxRM  PTRATIOxAGE  PTRATIOxDIS  PTRATIOxRAD  PTRATIOxTAX  \\\n",
       "220    120.9474      1539.90     49.79358        139.2       5341.8   \n",
       "71     114.4512       336.00    101.51616         76.8       5856.0   \n",
       "240    114.4902       901.38    105.17926         99.6       4980.0   \n",
       "6       91.3824      1012.32     84.51960         76.0       4727.2   \n",
       "417    107.1408      1799.82     33.27950        484.8      13453.2   \n",
       "\n",
       "     PTRATIOxPTRATIO  PTRATIOxB  PTRATIOxLSTAT       BxCRIM     BxZN  \\\n",
       "220           302.76   6815.580        168.954   140.263853      0.0   \n",
       "71            368.64   7237.248        189.696    59.842994      0.0   \n",
       "240           275.56   6494.750        188.908    44.324713  11737.5   \n",
       "6             231.04   6013.120        188.936    34.927524   4945.0   \n",
       "417           408.04   2572.672        538.128  3303.794816      0.0   \n",
       "\n",
       "       BxINDUS  BxCHAS      BxNOX        BxRM      BxAGE        BxDIS  \\\n",
       "220  2428.5400   391.7  198.59190  2722.70670  34665.450  1120.927890   \n",
       "71   4074.7214     0.0  155.67622  2246.93934   6596.450  1992.994862   \n",
       "240  1928.8625     0.0  167.45500  2698.45125  21244.875  2478.999125   \n",
       "6    3113.3720     0.0  207.29440  2378.34720  26346.960  2199.733800   \n",
       "417  2305.2160     0.0   86.47744   675.51744  11347.776   209.825600   \n",
       "\n",
       "       BxRAD      BxTAX  BxPTRATIO          BxB    BxLSTAT  LSTATxCRIM  \\\n",
       "220  3133.60  120251.90   6815.580  153428.8900  3803.4070    3.477054   \n",
       "71   1507.76  114966.70   7237.248  142083.7636  3724.1672    1.568549   \n",
       "240  2347.50  117375.00   6494.750  153076.5625  4452.4250    1.289240   \n",
       "6    1978.00  123031.60   6013.120  156499.3600  4917.3080    1.097445   \n",
       "417  3056.64   84821.76   2572.672   16220.5696  3392.8704  691.057584   \n",
       "\n",
       "     LSTATxZN  LSTATxINDUS  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  \\\n",
       "220     0.000      60.2020        9.71    4.92297   67.49421    859.335   \n",
       "71      0.000     106.8028        0.00    4.08044   58.89468    172.900   \n",
       "240   341.400      56.1034        0.00    4.87064   78.48786    617.934   \n",
       "6     155.375      97.8241        0.00    6.51332   74.72916    827.838   \n",
       "417     0.000     482.1840        0.00   18.08856  141.29856   2373.624   \n",
       "\n",
       "     LSTATxDIS  LSTATxRAD  LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220  27.787107      77.68    2980.97        168.954  3803.4070      94.2841  \n",
       "71   52.238524      39.52    3013.40        189.696  3724.1672      97.6144  \n",
       "240  72.104818      68.28    3414.00        188.908  4452.4250     129.5044  \n",
       "6    69.117015      62.15    3865.73        188.936  4917.3080     154.5049  \n",
       "417  43.889400     639.36   17742.24        538.128  3392.8704     709.6896  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af23098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 182)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b7cc4",
   "metadata": {},
   "source": [
    "### 특성확장된 train 데이터의 값과 정답으로 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2afe006",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression().fit(extended_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "010dacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_test = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa974ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\2255345048.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n"
     ]
    }
   ],
   "source": [
    "for col1 in x_train.columns : #13번 반복(컬럼 13개)\n",
    "    for col2 in x_train.columns : #13번 반복 더 해야 169개\n",
    "        extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0976679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 182)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4eb95a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102 entries, 329 to 108\n",
      "Columns: 182 entries, CRIM to LSTATxLSTAT\n",
      "dtypes: float64(182)\n",
      "memory usage: 145.8 KB\n"
     ]
    }
   ],
   "source": [
    "extended_x_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa23fb",
   "metadata": {},
   "source": [
    "### 오차 = 예측값 - 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "399faf0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_pre_train = lr2.predict(extended_x_train)\n",
    "ex_pre_test = lr2.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6532c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_mse = mean_squared_error(ex_pre_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69a3a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_test_mse = mean_squared_error(ex_pre_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33cba978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 4.340278052012249\n",
      "test mse: 31.277814972583755\n",
      "train, test mse의 차이: -26.937536920571507\n",
      "train, test rmse의 차이: -3.509323838030567\n"
     ]
    }
   ],
   "source": [
    "print('train mse:', ex_train_mse)\n",
    "print('test mse:', ex_test_mse)\n",
    "print('train, test mse의 차이:', ex_train_mse - ex_test_mse)\n",
    "print('train, test rmse의 차이:',ex_train_mse**0.5 - ex_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e196a140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158858583939284"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#r^2\n",
    "lr2.score(extended_x_train, y_train)\n",
    "lr2.score(extended_x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c12ff522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터의 설명력이 높으니 과대적합의 가능성이 있겠다!\n",
    "# --> 규제(랏쏘, 릿지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6e57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cdfc822",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0564fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7481f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha = 10).fit(extended_x_train, y_train) #규제의 강도(높을수록 과대적합 줄어듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5220fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 특성의 수: 46\n"
     ]
    }
   ],
   "source": [
    "print('사용한 특성의 수:', np.sum(lasso.coef_ != 0)) #가중치가 0이 아닌 값 추출해서 더해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20ed1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_lasso = lasso.predict(extended_x_train)\n",
    "pre_test_lasso = lasso.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0554666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오차확인\n",
    "lasso_train_mse = mean_squared_error(pre_train_lasso, y_train)\n",
    "lasso_test_mse = mean_squared_error(pre_test_lasso, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84c85a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 9.825814145224081\n",
      "test mse: 26.404674056715386\n",
      "train, test 차이: -16.578859911491307\n"
     ]
    }
   ],
   "source": [
    "print('train mse:',lasso_train_mse)\n",
    "print('test mse:',lasso_test_mse)\n",
    "print('train, test 차이:', lasso_train_mse - lasso_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ed4a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 3.1346154700734954\n",
      "test rmse: 5.138547854862829\n",
      "train, test rmse 차이: -2.003932384789334\n"
     ]
    }
   ],
   "source": [
    "print('train rmse:', lasso_train_mse**0.5)\n",
    "print('test rmse:', lasso_test_mse**0.5)\n",
    "print('train, test rmse 차이:', lasso_train_mse**0.5 - lasso_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85ee38b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8845973124097618"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_train, y_train) #모델이 만든 직선이 train데이터를 88% 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6ab7a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6757315458712968"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_test, y_test) #훈련데이터 설명력만 높으니 과대적합 가능성 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba81a9e",
   "metadata": {},
   "source": [
    "### 릿지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d060bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d289cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10).fit(extended_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "633523cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 특성의 수: 182\n"
     ]
    }
   ],
   "source": [
    "print('사용한 특성의 수:',np.sum(ridge.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4093114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_ridge = ridge.predict(extended_x_train)\n",
    "pre_test_ridge = ridge.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "947e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_mse = mean_squared_error(pre_train_ridge, y_train)\n",
    "ridge_test_mse = mean_squared_error(pre_test_ridge, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5e92282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 5.09456324639931\n",
      "test mse: 26.79259651669547\n",
      "train, test mse 차이: -21.698033270296158\n",
      "train rmse: 2.25711391967692\n",
      "test rmse: 5.176156539044725\n",
      "train, test rmse 차이: -2.919042619367805\n"
     ]
    }
   ],
   "source": [
    "print('train mse:', ridge_train_mse)\n",
    "print('test mse:', ridge_test_mse)\n",
    "print('train, test mse 차이:',ridge_train_mse - ridge_test_mse)\n",
    "\n",
    "print('train rmse:', ridge_train_mse ** 0.5)\n",
    "print('test rmse:', ridge_test_mse**0.5)\n",
    "print('train, test rmse 차이:', ridge_train_mse**0.5 - ridge_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca28e386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401651321668143"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_train, y_train) # train 데이터에 대해서 릿지모델이 94% 설명력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e56ec283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6709675780923591"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 알파값일때 ridge보다는 lasso에서 과대적합을 더 잘 해소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2f567",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝\n",
    "- alpha 값을 바꿔가면서 rmse 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723adb8d",
   "metadata": {},
   "source": [
    "#### 랏쏘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9e88bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001,0.01,0.1,1,10,100,1000]\n",
    "lasso_train_list = [] #train 데이터의 rmse값을 담아줄 빈 리스트\n",
    "lasso_test_list = [] #test데이터 rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7a011765",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.188e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.870e+01, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "for i in alpha_list:\n",
    "    lasso = Lasso(alpha = i)\n",
    "    lasso.fit(extended_x_train, y_train)\n",
    "    \n",
    "    # train 데이터 예측\n",
    "    train_pred = lasso.predict(extended_x_train)\n",
    "    lasso_train_rmse = mean_squared_error(train_pred, y_train)**0.5\n",
    "    lasso_train_list.append(lasso_train_rmse)\n",
    "    \n",
    "    # test 데이터도 똑같이\n",
    "    test_pred = lasso.predict(extended_x_test)\n",
    "    lasso_test_rmse = mean_squared_error(test_pred, y_test)**0.5\n",
    "    lasso_test_list.append(lasso_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe8540de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2758518221817816,\n",
       " 2.332152582439725,\n",
       " 2.556464318296377,\n",
       " 2.8421497511641136,\n",
       " 3.1346154700734954,\n",
       " 3.9961466355356023,\n",
       " 5.353949685680336]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e3df6e66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.795351938140569,\n",
       " 5.447613887368501,\n",
       " 4.676403047411522,\n",
       " 5.053153764089218,\n",
       " 5.138547854862829,\n",
       " 5.737963242918281,\n",
       " 6.985827230756549]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "324fe3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "plt.rc('font', family=\"Malgun Gothic\")\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef93d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(lasso_train_list, label='train')\n",
    "plt.plot(lasso_test_list, label='test')\n",
    "plt.title('Lasso 모델 alpha - RMSE 그래프')\n",
    "plt.xlabel('alpha 값')\n",
    "plt.xticks(np.arange(7), alpha_list) #x축 범위 설정(x축 값의 개수, 나타낼 값), xticks없으면 배열넘버로 표기\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend() #범례(왼쪽옆에 색상표시)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e76af",
   "metadata": {},
   "source": [
    "#### 릿지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d4fff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list2 = [0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_list = []\n",
    "ridge_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d5a17863",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\orange3-\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.05867e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "for i in alpha_list2:\n",
    "    ridge = Ridge(alpha=i).fit(extended_x_train, y_train)\n",
    "    \n",
    "    #train 예측\n",
    "    train_pred = ridge.predict(extended_x_train) #학습시킨 ridge로 예측해봐\n",
    "    ridge_train_rmse = mean_squared_error(train_pred, y_train)**0.5\n",
    "    ridge_train_list.append(ridge_train_rmse)\n",
    "    \n",
    "    test_pred = ridge.predict(extended_x_test) #예측값 얻어야\n",
    "    ridge_test_rmse = mean_squared_error(test_pred, y_test)**0.5\n",
    "    ridge_test_list.append(ridge_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32a32ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge 모델 train, test rmse 차이: -2.2821610893355504\n",
      "Lasso 모델 train, test rmse 차이: -1.6318775450762137\n"
     ]
    }
   ],
   "source": [
    "print('Ridge 모델 train, test rmse 차이:', ridge_train_rmse - ridge_test_rmse)\n",
    "print('Lasso 모델 train, test rmse 차이:', lasso_train_rmse - lasso_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7955abb4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0835065507717947,\n",
       " 2.0850764409779257,\n",
       " 2.0994432636573346,\n",
       " 2.1580820650757446,\n",
       " 2.25711391967692,\n",
       " 2.4260271435858827,\n",
       " 2.599646390917805]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9eeb314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.653502668471952,\n",
       " 5.874871100553151,\n",
       " 6.0103239800468335,\n",
       " 5.783909816750169,\n",
       " 5.176156539044725,\n",
       " 4.765046455866838,\n",
       " 4.8818074802533555]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4cd7ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "plt.rc('font', family=\"Malgun Gothic\") #한글인코딩\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9d72cf25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(ridge_train_list, label='train')\n",
    "plt.plot(ridge_test_list, label='test')\n",
    "plt.title('Ridge 모델 alpha - rmse')\n",
    "plt.xlabel('alpha')\n",
    "plt.xticks(np.arange(7), alpha_list)\n",
    "plt.ylabel('rmse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bc654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
